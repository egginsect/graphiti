# Graphiti MCP Server Environment Configuration

# Neo4j Database Configuration
# These settings are used to connect to your Neo4j database
# For Docker: use bolt://neo4j:7687, for local: use bolt://localhost:7687
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password_here

# LLM API Configuration
# Required for LLM operations - supports OpenAI, Gemini, and Azure OpenAI

# OpenAI Configuration (default)
OPENAI_API_KEY=your_openai_api_key_here
MODEL_NAME=gpt-4o-mini
EMBEDDER_MODEL_NAME=text-embedding-3-small

# Gemini Configuration (alternative to OpenAI)
# To use Gemini, set these values and use the OpenAI-compatible endpoint
# OPENAI_API_KEY=your_gemini_api_key_here
# OPENAI_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
# MODEL_NAME=gemini-2.0-flash
# EMBEDDER_MODEL_NAME=gemini-embedding-001

# Optional: Group ID for namespacing graph data
# GROUP_ID=my_project

# Optional: Rate limiting for API calls (default: 10)
SEMAPHORE_LIMIT=3

# Optional: Path configuration for Docker
# PATH=/root/.local/bin:${PATH}

# Azure OpenAI configuration
# Optional: Only needed for Azure OpenAI endpoints
# AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint_here
# AZURE_OPENAI_API_VERSION=2025-01-01-preview
# AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-gpt-4o-mini-deployment
# AZURE_OPENAI_EMBEDDING_API_VERSION=2023-05-15
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME=text-embedding-3-large-deployment
# AZURE_OPENAI_USE_MANAGED_IDENTITY=false
